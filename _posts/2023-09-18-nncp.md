---
layout: post
title: 小白笔记 - NNCP无损压缩算法
author: enh6
categories: blog
---

[NNCP](https://bellard.org/nncp/)是19年Fabrice Bellard发布的用神经网络模型无损压缩数据的项目，记得以前在网上看到过，但没仔细看到底是什么原理。

前几天又刷到了[Hutter Prize](http://prize.hutter1.net/)：如果能无损压缩一个1GB的Wikipedia数据集`enwik9`，并且压缩率能再提升1%，就能得到5000欧的悬赏。一看到money我就来兴趣了啊，就认真的看了一下。

然后发现目前最好成绩是能压缩到114MB，然后又发现NNCP居然能直接干到108MB！只是速度太慢达不到Hutter Prize要求。

于是我就想了解一下NNCP的原理，在一个[网上评论](https://news.ycombinator.com/item?id=27244810)的帮助下，居然似乎看明白了！而且这个方法简单，思路惊奇，当时我就震惊了！给我一百个脑子也想不出来。

然后网上搜了一下，没有什么中文的NNCP介绍，但搜到了一篇相关文章[《实现通用人工智能的可能道路：无损压缩！》](https://zhuanlan.zhihu.com/p/651212186)，把原理说明白了，而且和大模型，OpenAI，AGI什么的联系起来了。原来就是前段时间一个OpenAI的人的分享，*Compression for AGI*，讲的似乎是一个原理。我只能说当时就看过一个解说文章[《世界的参数倒影：为何GPT通过Next Token Prediction可以产生智能》](https://zhuanlan.zhihu.com/p/632795115)，但就硬是没看懂。。

下面我也描述一下自己对这个算法的乞丐级理解：

- 压缩算法本身是根据字符出现的概率分布，用算数编码来压缩。而这个概率是动态的，是一个AI模型的输出，模型相当于预测next token。在一个字符一个字符编码压缩的同时，同时一个字符一个字符的训练这个模型。压缩过程就是模型训练过程，模型经过训练，预测的更准了，压缩率也就更高。最后得到压缩后的数据和一个训练后的模型。
- 算法妙的地方在于，训练好的模型不使用也不保存，解压缩的过程依然是从头一个字符一个字符的训练模型，只要保证模型训练过程和压缩的时候完全一模一样，每次模型输出的概率就一样，就可以用来解压缩。
- 这样压缩率就和模型的大小无关了，可以搞一个参数量很大的transformer模型来用。而一个epoch训练的效果越好，压缩率越高。
- 所以压缩或者解压就是模型训练过程，所以速度很慢。

不知道理解的对不对，有没有说清楚。最后再次赞叹一句：妙啊！